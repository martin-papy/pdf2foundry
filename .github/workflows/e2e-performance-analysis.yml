name: "Performance Analysis"

on:
  workflow_call:
    inputs:
      tier2_run_id:
        description: "Run ID from Tier 2 tests for artifact download"
        required: false
        type: string
      tier3_run_id:
        description: "Run ID from Tier 3 tests for artifact download"
        required: false
        type: string
    outputs:
      baseline_missing:
        description: "Whether baseline performance data is missing"
        value: ${{ jobs.performance-check.outputs.baseline_missing }}
      regression_detected:
        description: "Whether performance regressions were detected"
        value: ${{ jobs.performance-check.outputs.regression_detected }}
      no_perf_data:
        description: "Whether no performance data was available"
        value: ${{ jobs.performance-check.outputs.no_perf_data }}
      
  workflow_dispatch:
    inputs:
      tier2_run_id:
        description: "Run ID from Tier 2 tests for artifact download"
        required: false
        type: string
      tier3_run_id:
        description: "Run ID from Tier 3 tests for artifact download"
        required: false
        type: string

env:
  PYTHONUNBUFFERED: 1

permissions:
  contents: read

jobs:
  performance-check:
    name: "Performance Regression Analysis"
    runs-on: ubuntu-latest
    timeout-minutes: 10
    
    outputs:
      baseline_missing: ${{ steps.perf-check.outputs.baseline_missing || 'false' }}
      regression_detected: ${{ steps.perf-check.outputs.regression_detected || 'false' }}
      no_perf_data: ${{ steps.perf-check.outputs.no_perf_data || 'true' }}
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        
      - name: Download Tier 2 artifacts
        if: inputs.tier2_run_id != ''
        continue-on-error: true
        uses: actions/download-artifact@v4
        with:
          name: tier2-reports-${{ inputs.tier2_run_id }}
          path: reports/tier2/
          
      - name: Download Tier 3 artifacts
        if: inputs.tier3_run_id != ''
        continue-on-error: true
        uses: actions/download-artifact@v4
        with:
          name: tier3-reports-${{ inputs.tier3_run_id }}
          path: reports/tier3/
          
      - name: Set up Python for performance analysis
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'
          
      - name: Cache pip dependencies
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('**/pyproject.toml') }}
          restore-keys: |
            ${{ runner.os }}-pip-
          
      - name: Install dependencies for performance analysis
        run: |
          python -m pip install --upgrade pip
          pip install -e ".[dev]"
          
      - name: Check performance regression (Environment-Aware)
        id: perf-check
        run: |
          python << 'EOF'
          import json
          import os
          import sys
          import platform
          from pathlib import Path
          
          # Environment detection for GitHub Actions
          def detect_ci_environment():
              return {
                  "environment_type": "github_actions",
                  "performance_tier": "slow",
                  "cpu_count": 2,  # GitHub Actions standard
                  "memory_gb": 7.0,  # GitHub Actions standard
                  "platform": platform.platform(),
                  "is_ci": True,
                  "is_github_actions": True,
              }
          
          def get_environment_specific_threshold():
              # GitHub Actions gets higher threshold due to variability
              return 0.4  # 40% instead of 20%
          
          def get_performance_multiplier():
              # GitHub Actions is expected to be ~4x slower than local MacBook Pro
              return 4.0
          
          env_info = detect_ci_environment()
          threshold = get_environment_specific_threshold()
          
          print(f"üîç Environment: {env_info['environment_type']} ({env_info['performance_tier']})")
          print(f"üìä Regression threshold: {threshold:.1%}")
          print(f"‚ö° Expected performance multiplier: {get_performance_multiplier():.1f}x")
          print()
          
          # Look for performance files in both tier2 and tier3 reports
          perf_files = []
          for tier_dir in ['reports/tier2', 'reports/tier3']:
              perf_dir = Path(tier_dir) / 'tests' / 'e2e' / 'perf'
              if perf_dir.exists():
                  perf_files.extend(perf_dir.glob('*.json'))
          
          current_file = None
          for f in perf_files:
              if f.name == 'latest.json':
                  current_file = f
                  break
          
          # Check if current performance file exists
          if not current_file or not current_file.exists():
              print("‚ùå Current performance file not found")
              with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
                  f.write("baseline_missing=false\n")
                  f.write("regression_detected=false\n")
                  f.write("no_perf_data=true\n")
              sys.exit(0)
          
          # Load current results
          with open(current_file) as f:
              current = json.load(f)
          
          # Look for environment-specific baselines first, then legacy baseline
          baseline_files = [
              Path('tests/e2e/perf/baseline_github_actions_slow.json'),  # Environment-specific
              Path('tests/e2e/perf/baseline.json')  # Legacy fallback
          ]
          
          baseline = None
          baseline_source = None
          
          for baseline_file in baseline_files:
              if baseline_file.exists():
                  try:
                      with open(baseline_file) as f:
                          baseline = json.load(f)
                      baseline_source = baseline_file.name
                      break
                  except (OSError, json.JSONDecodeError):
                      continue
          
          if baseline is None:
              print("‚ö†Ô∏è  No baseline performance file found - skipping regression check")
              with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
                  f.write("baseline_missing=true\n")
                  f.write("regression_detected=false\n")
                  f.write("no_perf_data=false\n")
              sys.exit(0)
          
          print(f"üìÅ Using baseline: {baseline_source}")
          
          # Compare performance metrics with environment awareness
          regressions = []
          summary_data = []
          
          for test_name in current.keys():
              if test_name.startswith('_'):  # Skip metadata
                  continue
                  
              if test_name not in baseline:
                  continue
                  
              current_metrics = current[test_name].get('metrics', {})
              baseline_metrics = baseline[test_name].get('metrics', {})
              
              for metric_name in current_metrics.keys():
                  if metric_name not in baseline_metrics:
                      continue
                      
                  current_val = current_metrics[metric_name].get('latest', 0)
                  baseline_val = baseline_metrics[metric_name].get('latest', 0)
                  
                  # If using legacy baseline, adjust for environment differences
                  if baseline_source == 'baseline.json':
                      baseline_val = baseline_val * get_performance_multiplier()
                  
                  if baseline_val > 0:
                      delta = (current_val - baseline_val) / baseline_val
                      summary_data.append({
                          'test': test_name,
                          'metric': metric_name,
                          'baseline': baseline_val,
                          'current': current_val,
                          'delta': delta,
                          'threshold': threshold,
                          'environment': env_info['environment_type'],
                          'baseline_source': baseline_source
                      })
                      
                      if delta > threshold:
                          regressions.append(f"{test_name}.{metric_name}: {delta:.1%} regression (threshold: {threshold:.1%})")
                      
                      print(f"üìä {test_name}.{metric_name}: {current_val:.1f}s vs {baseline_val:.1f}s ({delta:+.1%})")
          
          # Write summary data for PR comment
          with open('perf_summary.json', 'w') as f:
              json.dump(summary_data, f, indent=2)
          
          # Set outputs
          with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
              f.write("baseline_missing=false\n")
              f.write(f"regression_detected={'true' if regressions else 'false'}\n")
              f.write(f"regression_count={len(regressions)}\n")
              f.write("no_perf_data=false\n")
          
          # Print results
          if regressions:
              print()
              print("‚ùå Performance regressions detected:")
              for regression in regressions:
                  print(f"  - {regression}")
              print()
              print("üí° Note: Regressions are compared against environment-appropriate baselines")
              # Don't fail the job for performance regressions, just report them
          else:
              print()
              print("‚úÖ No performance regressions detected")
              print("üí° Performance is within expected range for GitHub Actions environment")
          EOF
          
      - name: Upload performance summary
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: performance-summary-${{ github.run_id }}
          path: perf_summary.json
          retention-days: 30
