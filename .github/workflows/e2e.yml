name: "E2E & Performance"

on:
  pull_request:
    types: [opened, synchronize, reopened]
  push:
    branches: [main]
  workflow_dispatch:

env:
  PYTHONUNBUFFERED: 1
  PIP_DISABLE_PIP_VERSION_CHECK: 1
  HF_HOME: ~/.cache/huggingface

permissions:
  contents: read
  pull-requests: write
  actions: read

jobs:
  e2e:
    name: "e2e"
    runs-on: ubuntu-latest
    timeout-minutes: 120
    
    concurrency:
      group: e2e-${{ github.ref }}
      cancel-in-progress: true
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'
      
      - name: Set up Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '24'
      
      - name: Install OS dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y tesseract-ocr libtesseract-dev poppler-utils
          tesseract --version
      
      - name: Cache pip dependencies
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('**/pyproject.toml') }}
          restore-keys: |
            ${{ runner.os }}-pip-
      
      - name: Cache HuggingFace models
        uses: actions/cache@v4
        with:
          path: ~/.cache/huggingface
          key: ${{ runner.os }}-hf-${{ hashFiles('tests/e2e/**/*.py') }}
          restore-keys: |
            ${{ runner.os }}-hf-
      
      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e ".[dev]"
      
      - name: System diagnostics
        run: |
          echo "=== System Information ==="
          python --version
          node --version
          tesseract --version
          echo "=== Disk Usage ==="
          df -h
          echo "=== Python Packages ==="
          pip list | grep -E "(pytest|docling|transformers|torch)"
      
      - name: Resolve PDF2FOUNDRY_CLI path
        run: |
          CLI_PATH=$(python -c "import shutil; print(shutil.which('pdf2foundry') or 'pdf2foundry')")
          echo "PDF2FOUNDRY_CLI=$CLI_PATH" >> $GITHUB_ENV
          echo "Resolved CLI path: $CLI_PATH"
      
      - name: Create reports directory
        run: mkdir -p reports
      
      - name: Run E2E tests (CI subset)
        env:
          PDF2FOUNDRY_CLI: ${{ env.PDF2FOUNDRY_CLI }}
          PERF_THRESHOLD: '0.2'
        run: |
          pytest tests/e2e -m "not slow and not perf" -n auto --junitxml=reports/junit.xml --durations=20 -q
      
      - name: Run performance and slow tests (scheduled only)
        if: github.event_name == 'schedule'
        env:
          PDF2FOUNDRY_CLI: ${{ env.PDF2FOUNDRY_CLI }}
          PERF_THRESHOLD: '0.2'
        run: |
          pytest tests/e2e -m "perf or slow" -n 1 -q
      
      - name: Upload test artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: e2e-reports
          path: |
            reports/**
            tests/e2e/perf/*.json
            .pytest_cache/**
      
      - name: Check performance regression
        if: always()
        id: perf-check
        run: |
          python << 'EOF'
          import json
          import os
          import sys
          from pathlib import Path
          
          # Configuration
          PERF_THRESHOLD = float(os.environ.get('PERF_THRESHOLD', '0.2'))
          current_file = Path('tests/e2e/perf/latest.json')
          baseline_file = Path('tests/e2e/perf/baseline.json')
          
          # Check if current performance file exists
          if not current_file.exists():
              print("‚ùå Current performance file not found")
              sys.exit(1)
          
          # Load current results
          with open(current_file) as f:
              current = json.load(f)
          
          # Check if baseline exists
          if not baseline_file.exists():
              print("‚ö†Ô∏è  Baseline performance file not found - skipping regression check")
              with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
                  f.write("baseline_missing=true\n")
                  f.write("regression_detected=false\n")
              sys.exit(0)
          
          # Load baseline results
          with open(baseline_file) as f:
              baseline = json.load(f)
          
          # Compare performance metrics
          regressions = []
          summary_data = []
          
          for test_name in current.keys():
              if test_name not in baseline:
                  continue
                  
              current_metrics = current[test_name].get('metrics', {})
              baseline_metrics = baseline[test_name].get('metrics', {})
              
              for metric_name in current_metrics.keys():
                  if metric_name not in baseline_metrics:
                      continue
                      
                  current_val = current_metrics[metric_name].get('latest', 0)
                  baseline_val = baseline_metrics[metric_name].get('latest', 0)
                  
                  if baseline_val > 0:
                      delta = (current_val - baseline_val) / baseline_val
                      summary_data.append({
                          'test': test_name,
                          'metric': metric_name,
                          'baseline': baseline_val,
                          'current': current_val,
                          'delta': delta,
                          'threshold': PERF_THRESHOLD
                      })
                      
                      if delta > PERF_THRESHOLD:
                          regressions.append(f"{test_name}.{metric_name}: {delta:.1%} regression (threshold: {PERF_THRESHOLD:.1%})")
          
          # Write summary data for PR comment
          with open('perf_summary.json', 'w') as f:
              json.dump(summary_data, f, indent=2)
          
          # Set outputs
          with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
              f.write("baseline_missing=false\n")
              f.write(f"regression_detected={'true' if regressions else 'false'}\n")
              f.write(f"regression_count={len(regressions)}\n")
          
          # Print results
          if regressions:
              print("‚ùå Performance regressions detected:")
              for regression in regressions:
                  print(f"  - {regression}")
              sys.exit(1)
          else:
              print("‚úÖ No performance regressions detected")
          EOF
      
      - name: Comment PR with performance summary
        if: github.event_name == 'pull_request' && always()
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const path = require('path');
            
            // Check if summary file exists
            const summaryFile = 'perf_summary.json';
            if (!fs.existsSync(summaryFile)) {
              console.log('No performance summary found');
              return;
            }
            
            // Load performance data
            const perfData = JSON.parse(fs.readFileSync(summaryFile, 'utf8'));
            const baselineMissing = '${{ steps.perf-check.outputs.baseline_missing }}' === 'true';
            const regressionDetected = '${{ steps.perf-check.outputs.regression_detected }}' === 'true';
            
            // Build markdown table
            let comment = '## üöÄ Performance Test Results\n\n';
            
            if (baselineMissing) {
              comment += '‚ö†Ô∏è **Baseline missing** - Performance comparison skipped\n\n';
            } else if (perfData.length === 0) {
              comment += 'üìä No performance metrics to compare\n\n';
            } else {
              const status = regressionDetected ? '‚ùå **Regressions detected**' : '‚úÖ **No regressions**';
              comment += `${status}\n\n`;
              
              comment += '| Test | Metric | Baseline | Current | Delta | Status |\n';
              comment += '|------|--------|----------|---------|-------|--------|\n';
              
              perfData.forEach(item => {
                const deltaPercent = (item.delta * 100).toFixed(1);
                const deltaSign = item.delta > 0 ? '+' : '';
                const status = item.delta > item.threshold ? '‚ùå' : '‚úÖ';
                const baseline = item.baseline.toFixed(3);
                const current = item.current.toFixed(3);
                
                comment += `| ${item.test} | ${item.metric} | ${baseline}s | ${current}s | ${deltaSign}${deltaPercent}% | ${status} |\n`;
              });
              
              comment += `\n**Threshold:** ${(perfData[0]?.threshold * 100 || 20).toFixed(0)}%\n`;
            }
            
            // Post comment
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: comment
            });
      
      - name: Annotate test failures
        if: failure()
        uses: mikepenz/action-junit-report@v4
        with:
          report_paths: 'reports/junit.xml'
